{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb0cb047",
   "metadata": {},
   "source": [
    "<font size = '5'> <center> <u>**Tick tack toe using Reinforcement Learning**</u></center></font>\n",
    "\n",
    "In this notebook, we will teach an agent to play tick tack toe using reinforcement learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38ab9f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first import all the libraries needed\n",
    "from board import tick_tack_toe\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4107b8",
   "metadata": {},
   "source": [
    "First we will check the board out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca92262e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |    |   \n",
      "    |    |  \n",
      "____|____|___\n",
      "    |    |  \n",
      "    |    |  \n",
      "____|____|___\n",
      "    |    |  \n",
      "    |    |  \n",
      "    |    |  \n"
     ]
    }
   ],
   "source": [
    "#initialize a game\n",
    "game = tick_tack_toe()\n",
    "game.print_board()"
   ]
  },
  {
   "attachments": {
    "pic1.PNG": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "ac521258",
   "metadata": {},
   "source": [
    "<u><font size = '3'> **Reinforcement Learning** </font></u>\n",
    "\n",
    "The model will give values to the agent after each state. The state consist all the moves played by the agents at the end of a game. During training, agent 1 will be taught to play first and agent 2 second. In agent 1's memory we will store all the states that were rewarding to agent 1 and in agent 2's memory we will store all the states that were rewarding for agent 2. \n",
    "\n",
    "Agent 1 will be playing 'O' and agent 2 will be playing 'X'. An example of a state vector is below. \n",
    "\n",
    "![pic1.PNG](attachment:pic1.PNG)\n",
    "\n",
    "In the game above, agent 1 started first so the following state vector will be assigned to agent 1's memory with a reward of +1. \n",
    "\n",
    "state = [1, 4, 5, 8, 9]\n",
    "\n",
    "since this is in player 1's memory 1 the player knows that the moves he/she played was 1, 5, 9 against moves 4 and 8 by agent 2. And since agent 1 won the game this will be a rewarding state for agent 1 whereas for agent 2, this will be a bad state to visit again. So we will assign -1 for agent 2 for this game. If the game is a draw both agents will recieve 0 points. \n",
    "\n",
    "So if the first iteration had the game above, player 1 and player 2 will have the following states stored. \n",
    "\n",
    "- Player 1 - [1, 4, 5, 8, 9] with a value of +1\n",
    "- Player 2 - [1, 4, 5, 8, 9] with a value of -1\n",
    "\n",
    "\n",
    "Now let's create some functions to select moves, store values etc.\n",
    "\n",
    "<u>Selecting moves -</u> \n",
    "\n",
    "Each agent will select moves based on the values of the stored moves in each memory. If there is no games stored in the memory the agent will randomly select moves. The agent also want to explore so we will have the agent select random moves once in a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dad822b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the first function will select moves\n",
    "\n",
    "def choose_move(move_number, avail_moves, current_state, states, values, epsilon = 0.1):\n",
    "    \"\"\"\n",
    "    inputs\n",
    "    -------\n",
    "    move_number - current move number (ranges from 1 - 9)\n",
    "    avail_moves - moves that are available to choose from\n",
    "    current_state - the current state vector of the game\n",
    "    states - memory with the already played states\n",
    "    values - an array containing the values of each of those states in the memory\n",
    "    epsilon - the probability the algorithm will explore new moves\n",
    "    \n",
    "    output\n",
    "    -------\n",
    "    action - this function outputs the most rewarding move\n",
    "    \"\"\"\n",
    "    \n",
    "    #exploration. choose 0 and 1 with a given probability. and explore if 0 is chosen\n",
    "    explore = np.random.choice([0, 1], p = [epsilon, 1 - epsilon])\n",
    "    \n",
    "    if explore == 0:\n",
    "        \n",
    "        #randomly choose action\n",
    "        action = np.random.choice(avail_moves)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #get all the indices from the states that is equal to the current state of the game\n",
    "        v = []\n",
    "        idx = []\n",
    "                   \n",
    "        for i, state in enumerate(states):\n",
    "            st = state[:move_number]\n",
    "            \n",
    "            if st == current_state:\n",
    "                v.append(values[i])          \n",
    "                idx.append(i)\n",
    "                \n",
    "        #find the maximum value\n",
    "        if (v == []) == True:\n",
    "            action = np.random.choice(avail_moves)\n",
    "            \n",
    "        else:\n",
    "            max_val_idx = np.argmax(v)\n",
    "            max_idx = idx[max_val_idx]\n",
    "            action = states[max_idx][move_number]\n",
    "            \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d733d13",
   "metadata": {},
   "source": [
    "We'll run some hypothetical situations to see if the choose_move function is working properly. We will use two example situations and will run multile iterations and plot the moves selected by the function. \n",
    "\n",
    "First we will create some hypothetical states and their values vectors. \n",
    "\n",
    "states = [[3, 1, 4, 5, 6], [1, 2, 5, 6, 4], [1, 2, 3, 6]]\n",
    "\n",
    "values = [8, 9, 1]\n",
    "\n",
    "<u>**Scenario 1**</u>\n",
    "\n",
    "move_number = 1\n",
    "\n",
    "available_moves = [3, 6, 0, 1]\n",
    "\n",
    "current_state = [1]\n",
    "\n",
    "In this situation, the current state is 1 and there are two games in the memory (states vector) that we already played move 1 first. If we look at the values vector the respective values for those two states are 9 and 1, so ideally the function should choose the next move from the state that has the highest value. So the algorithm will choose move 2 frequently in this situation.\n",
    "\n",
    "<u>**Scenario 2**</u>\n",
    "\n",
    "move_number = 3\n",
    "\n",
    "available_moves = [3, 6, 0, 1]\n",
    "\n",
    "current_state = [1, 2]\n",
    "\n",
    "In this situation, the current state is [1, 2]. There are two moves played so the function will choose the third move. Out of states already played there are two states that 1 and 2 were played. Out of those two states the state [1, 2, 5, 6, 4] has the highest reward (9) so the algorithm will frequently choose move 5 to maximize reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "44ec09a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#states and values\n",
    "states = [[3, 1, 4, 5, 6], [1, 2, 5, 6, 4], [1, 2, 3, 6]]\n",
    "values = [8, 9, 1]\n",
    "\n",
    "#available moves\n",
    "available_moves =  [3, 6, 0, 1]\n",
    "\n",
    "#situation 1\n",
    "current_state = [1]\n",
    "move_number = len(current_state)\n",
    "\n",
    "selected_move = []\n",
    "\n",
    "for i in range(100):\n",
    "    selected_move.append(choose_move(move_number, available_moves, current_state, states, values))\n",
    "    \n",
    "#plot the distribution\n",
    "plt.figure(figsize = (5, 3))\n",
    "plt.hist(selected_move)\n",
    "plt.title('Scenario 1')\n",
    "plt.show()\n",
    "\n",
    "#situation 2\n",
    "current_state = [1, 2]\n",
    "move_number = len(current_state)\n",
    "\n",
    "selected_move = []\n",
    "\n",
    "for i in range(100):\n",
    "    selected_move.append(choose_move(move_number, available_moves, current_state, states, values))\n",
    "    \n",
    "#plot the distribution\n",
    "plt.figure(figsize = (5, 3))\n",
    "plt.hist(selected_move)\n",
    "plt.title('Scenario 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61747427",
   "metadata": {},
   "source": [
    "As expected the agent has selected move 2 in scenario 1 and move 5 in scenario 2. So the function seems to collect the moves correctly. \n",
    "\n",
    "Now let's create a function that rewards for winning states and penalizes losing states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "707340f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to give the rewards\n",
    "\n",
    "def give_reward(states_and_values, moves, reward, alpha = 0.5):\n",
    "    \n",
    "    \"\"\"\n",
    "    inputs\n",
    "    ---------\n",
    "    states_and_values - a dictionary containing states and each state's values\n",
    "    moves - a list containing the current_state moves\n",
    "    reward - the reward awarded \n",
    "    alpha - the learning_rate by default set to 0.5\n",
    "    \n",
    "    output\n",
    "    -------\n",
    "    states_and_values - a dictionary after the moves with the new reward added\n",
    "    \"\"\"\n",
    "    \n",
    "    if tuple(moves) in states_and_values.keys():\n",
    "        states_and_values[tuple(moves)] += alpha * (reward)\n",
    "    else:\n",
    "        states_and_values[tuple(moves)] = reward\n",
    "        \n",
    "    return states_and_values\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef11fb",
   "metadata": {},
   "source": [
    "Now Let's try to create a self play algorithm where two agents are playing together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c82fad37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |    |   \n",
      "    |    |  \n",
      "____|____|___\n",
      "    |    |  \n",
      " O  | O  |  \n",
      "____|____|___\n",
      "    |    |  \n",
      " X  | X  | X\n",
      "    |    |  \n",
      "    |    |   \n",
      " X  |    | O\n",
      "____|____|___\n",
      "    |    |  \n",
      " O  | O  | X\n",
      "____|____|___\n",
      "    |    |  \n",
      " O  | X  | X\n",
      "    |    |  \n",
      "    |    |   \n",
      " X  | X  | X\n",
      "____|____|___\n",
      "    |    |  \n",
      " X  |    | O\n",
      "____|____|___\n",
      "    |    |  \n",
      " O  |    | O\n",
      "    |    |  \n",
      "    |    |   \n",
      " O  | X  | X\n",
      "____|____|___\n",
      "    |    |  \n",
      "    | O  | X\n",
      "____|____|___\n",
      "    |    |  \n",
      " O  | X  | O\n",
      "    |    |  \n"
     ]
    }
   ],
   "source": [
    "#player 1 memory\n",
    "player1 = {}\n",
    "#states1 = []\n",
    "#values1 = []\n",
    "\n",
    "player2 = {}\n",
    "#states2 = []\n",
    "#values2 = []\n",
    "\n",
    "#number of iterations of self play\n",
    "num_its = 10**4\n",
    "\n",
    "\n",
    "for i in range(num_its):\n",
    "    \n",
    "    #player 1 is always first\n",
    "    player = 1\n",
    "    \n",
    "    #game_on\n",
    "    game_on = True\n",
    "      \n",
    "    #initialize the game\n",
    "    game = tick_tack_toe()\n",
    "\n",
    "    #intial values\n",
    "    move_number = 1\n",
    "    avail_moves = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    current_state = []\n",
    "    winner = ''\n",
    "\n",
    "    while move_number < 10:\n",
    "        \n",
    "        #if a winner is declared by the game.check_winner() the game will stop and will assign the rewards\n",
    "        if (winner == 'X') or (winner == 'O'):\n",
    "            #print the end game after every 2000th iteration\n",
    "            if i % 2000 == 0:\n",
    "                \n",
    "                game.print_board()\n",
    "\n",
    "            break\n",
    "        \n",
    "        #choose move for each player\n",
    "        \n",
    "        if player == 1:\n",
    "            move = choose_move(move_number, avail_moves, current_state, states1, values1, epsilon = 0.1)\n",
    "        \n",
    "        elif player == 2:\n",
    "            move = choose_move(move_number, avail_moves, current_state, states2, values2, epsilon = 0.1)\n",
    "        \n",
    "        #play the move\n",
    "        _, avail_moves = game.play(move, player)\n",
    "        winner = game.check_winner()\n",
    "        #game.print_board()\n",
    "        #clear_output()\n",
    "\n",
    "        #change the move number\n",
    "        move_number = move_number + 1\n",
    "        #append the move to the current state\n",
    "        current_state.append(move)\n",
    "        \n",
    "        #switch turns\n",
    "        if player == 1:\n",
    "            player = 2\n",
    "        elif player == 2:\n",
    "            player = 1\n",
    "    \n",
    "    if winner == 'O':\n",
    "        \n",
    "        #player 1\n",
    "        player1 = give_reward(player1, current_state, 1)        \n",
    "        states_tup1 = player1.keys()\n",
    "        states1 = [list(s) for s in states_tup1]\n",
    "        values1 = list(player1.values())\n",
    "              \n",
    "        #player 2\n",
    "        player2 = give_reward(player2, current_state, -1)\n",
    "        states_tup2 = player2.keys()\n",
    "        states2 = [list(s) for s in states_tup2]\n",
    "        values2 = list(player2.values())\n",
    "        \n",
    "    elif winner == 'X':\n",
    "        \n",
    "        #Player 2\n",
    "        player2 = give_reward(player2, current_state, 1)\n",
    "        states_tup2 = player2.keys()\n",
    "        states2 = [list(s) for s in states_tup2]\n",
    "        values2 = list(player2.values())\n",
    "    \n",
    "        #player 1\n",
    "        player1 = give_reward(player1, current_state, -1)\n",
    "        states_tup1 = player1.keys()\n",
    "        states1 = [list(s) for s in states_tup1]\n",
    "        values1 = list(player1.values())\n",
    "        \n",
    "        \n",
    "    elif winner == '':\n",
    "        print('draw')\n",
    "        #player 1\n",
    "        player1 = give_reward(player1, current_state, 0)\n",
    "        states_tup1 = player1.keys()\n",
    "        states1 = [list(s) for s in states_tup1]\n",
    "        values1 = list(player1.values())\n",
    "        \n",
    "        #player 2\n",
    "        player2 = give_reward(player2, current_state, 0)\n",
    "        states_tup2 = player2.keys()\n",
    "        states2 = [list(s) for s in states_tup2]\n",
    "        values2 = list(player2.values())\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "43b10f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(values1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec52903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
